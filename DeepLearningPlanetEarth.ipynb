{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code written following the TensorFlow text generation tutorial: https://www.tensorflow.org/tutorials/text/text_generation\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "print (\"TensorFlow version: \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 234825 characters\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/Charlie/Desktop/input.txt\"\n",
    "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A hundred years ago\n",
      "\n",
      "there were one and a half billion people on Earth.\n",
      "\n",
      "Now, over six billion crowd our fragile planet.\n",
      "\n",
      "But even so, there are still places barely touched by humanity.\n",
      "\n",
      "This series will take to the last wildernesses\n",
      "\n",
      "and show you th\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '\"' :   3,\n",
      "  '%' :   4,\n",
      "  \"'\" :   5,\n",
      "  '(' :   6,\n",
      "  ')' :   7,\n",
      "  ',' :   8,\n",
      "  '-' :   9,\n",
      "  '.' :  10,\n",
      "  '0' :  11,\n",
      "  '1' :  12,\n",
      "  '2' :  13,\n",
      "  '3' :  14,\n",
      "  '4' :  15,\n",
      "  '5' :  16,\n",
      "  '6' :  17,\n",
      "  '7' :  18,\n",
      "  '8' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'A hundred yea' ---- characters mapped to int ---- > [23  1 55 68 61 51 65 52 51  1 72 52 48]\n"
     ]
    }
   ],
   "source": [
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      " \n",
      "h\n",
      "u\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'A hundred years ago\\n\\nthere were one and a half billion people on Earth.\\n\\nNow, over six billion crowd '\n",
      "'our fragile planet.\\n\\nBut even so, there are still places barely touched by humanity.\\n\\nThis series wil'\n",
      "'l take to the last wildernesses\\n\\nand show you the planet and its wildlife\\n\\nas you have never seen the'\n",
      "'m before.\\n\\nImagine our world without sun.\\n\\nMale Emperor Penguins are facing the nearest that exists o'\n",
      "\"n planet Earth -\\n\\nwinter in Antarctica.\\n\\nIt's continuously dark\\n\\nand temperatures drop to minus seven\"\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'A hundred years ago\\n\\nthere were one and a half billion people on Earth.\\n\\nNow, over six billion crowd'\n",
      "Target data: ' hundred years ago\\n\\nthere were one and a half billion people on Earth.\\n\\nNow, over six billion crowd '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 23 ('A')\n",
      "  expected output: 1 (' ')\n",
      "Step    1\n",
      "  input: 1 (' ')\n",
      "  expected output: 55 ('h')\n",
      "Step    2\n",
      "  input: 55 ('h')\n",
      "  expected output: 68 ('u')\n",
      "Step    3\n",
      "  input: 68 ('u')\n",
      "  expected output: 61 ('n')\n",
      "Step    4\n",
      "  input: 61 ('n')\n",
      "  expected output: 51 ('d')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 77) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           19712     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 77)            78925     \n",
      "=================================================================\n",
      "Total params: 4,036,941\n",
      "Trainable params: 4,036,941\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 64, 72, 18, 34, 73, 73, 48,  6, 71, 38, 76, 37, 26, 60,  3,  7,\n",
       "       56, 10, 34, 20,  4, 19, 53, 14, 30, 72, 37, 54, 68,  8, 57, 24, 45,\n",
       "       54, 35, 67, 11, 31, 31, 13,  8, 73, 26, 37, 69, 14, 28,  2, 57, 34,\n",
       "       13, 38, 36, 46, 68, 69, 56, 50, 29, 64, 50, 48, 46, 74, 12,  0, 25,\n",
       "       18,  1, 63, 55, 22, 68, 37, 63, 41,  7, 34, 30, 57, 33, 47, 51, 69,\n",
       "        4, 11, 62, 58, 15, 76, 24, 21, 59, 11, 42, 27, 54, 14, 55])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " \"musk oxen,\\n\\nwhose entourage grows throughout the day.\\n\\nThis odd assembly of vegetarians\\ndoesn't go u\"\n",
      "\n",
      "Next Char Predictions: \n",
      " '\\'qy7Lzza(xP—ODm\")i.L9%8f3HyOgu,jBWgMt0II2,zDOv3F!jL2PNYuvicGqcaY{1\\nC7 ph?uOpS)LHjKZdv%0ok4—B:l0TEg3h'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 77)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.3446827\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "36/36 [==============================] - 162s 4s/step - loss: 3.4517\n",
      "Epoch 2/45\n",
      "36/36 [==============================] - 148s 4s/step - loss: 2.5403\n",
      "Epoch 3/45\n",
      "36/36 [==============================] - 141s 4s/step - loss: 2.2878\n",
      "Epoch 4/45\n",
      "36/36 [==============================] - 138s 4s/step - loss: 2.1768\n",
      "Epoch 5/45\n",
      "36/36 [==============================] - 151s 4s/step - loss: 2.0804\n",
      "Epoch 6/45\n",
      "36/36 [==============================] - 147s 4s/step - loss: 1.9862\n",
      "Epoch 7/45\n",
      "36/36 [==============================] - 154s 4s/step - loss: 1.9020\n",
      "Epoch 8/45\n",
      "36/36 [==============================] - 151s 4s/step - loss: 1.8196\n",
      "Epoch 9/45\n",
      "36/36 [==============================] - 150s 4s/step - loss: 1.7401\n",
      "Epoch 10/45\n",
      "36/36 [==============================] - 158s 4s/step - loss: 1.6632\n",
      "Epoch 11/45\n",
      "36/36 [==============================] - 166s 5s/step - loss: 1.5884\n",
      "Epoch 12/45\n",
      "36/36 [==============================] - 161s 4s/step - loss: 1.5185\n",
      "Epoch 13/45\n",
      "36/36 [==============================] - 146s 4s/step - loss: 1.4574\n",
      "Epoch 14/45\n",
      "36/36 [==============================] - 155s 4s/step - loss: 1.4015\n",
      "Epoch 15/45\n",
      "36/36 [==============================] - 160s 4s/step - loss: 1.3468\n",
      "Epoch 16/45\n",
      "36/36 [==============================] - 160s 4s/step - loss: 1.2988\n",
      "Epoch 17/45\n",
      "36/36 [==============================] - 152s 4s/step - loss: 1.2544\n",
      "Epoch 18/45\n",
      "36/36 [==============================] - 153s 4s/step - loss: 1.2091\n",
      "Epoch 19/45\n",
      "36/36 [==============================] - 158s 4s/step - loss: 1.1639\n",
      "Epoch 20/45\n",
      "36/36 [==============================] - 150s 4s/step - loss: 1.1185\n",
      "Epoch 21/45\n",
      "36/36 [==============================] - 157s 4s/step - loss: 1.0787\n",
      "Epoch 22/45\n",
      "36/36 [==============================] - 158s 4s/step - loss: 1.0300\n",
      "Epoch 23/45\n",
      "36/36 [==============================] - 158s 4s/step - loss: 0.9857\n",
      "Epoch 24/45\n",
      "36/36 [==============================] - 157s 4s/step - loss: 0.9397\n",
      "Epoch 25/45\n",
      "36/36 [==============================] - 167s 5s/step - loss: 0.8918\n",
      "Epoch 26/45\n",
      "36/36 [==============================] - 155s 4s/step - loss: 0.8448\n",
      "Epoch 27/45\n",
      "36/36 [==============================] - 153s 4s/step - loss: 0.7980\n",
      "Epoch 28/45\n",
      "36/36 [==============================] - 150s 4s/step - loss: 0.7469\n",
      "Epoch 29/45\n",
      "36/36 [==============================] - 150s 4s/step - loss: 0.7013\n",
      "Epoch 30/45\n",
      "36/36 [==============================] - 150s 4s/step - loss: 0.6520\n",
      "Epoch 31/45\n",
      "36/36 [==============================] - 152s 4s/step - loss: 0.6063\n",
      "Epoch 32/45\n",
      "36/36 [==============================] - 150s 4s/step - loss: 0.5633\n",
      "Epoch 33/45\n",
      "36/36 [==============================] - 150s 4s/step - loss: 0.5211\n",
      "Epoch 34/45\n",
      "36/36 [==============================] - 149s 4s/step - loss: 0.4883\n",
      "Epoch 35/45\n",
      "36/36 [==============================] - 161s 4s/step - loss: 0.4545\n",
      "Epoch 36/45\n",
      "36/36 [==============================] - 162s 4s/step - loss: 0.4208\n",
      "Epoch 37/45\n",
      "36/36 [==============================] - 157s 4s/step - loss: 0.3933\n",
      "Epoch 38/45\n",
      "36/36 [==============================] - 155s 4s/step - loss: 0.3719\n",
      "Epoch 39/45\n",
      "36/36 [==============================] - 154s 4s/step - loss: 0.3522\n",
      "Epoch 40/45\n",
      "36/36 [==============================] - 153s 4s/step - loss: 0.3306\n",
      "Epoch 41/45\n",
      "36/36 [==============================] - 154s 4s/step - loss: 0.3196\n",
      "Epoch 42/45\n",
      "36/36 [==============================] - 154s 4s/step - loss: 0.3041\n",
      "Epoch 43/45\n",
      "36/36 [==============================] - 156s 4s/step - loss: 0.2938\n",
      "Epoch 44/45\n",
      "36/36 [==============================] - 147s 4s/step - loss: 0.2816\n",
      "Epoch 45/45\n",
      "36/36 [==============================] - 156s 4s/step - loss: 0.2711\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_45'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            19712     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 77)             78925     \n",
      "=================================================================\n",
      "Total params: 4,036,941\n",
      "Trainable params: 4,036,941\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 3000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 0.6\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is now hidden blizzards,\n",
      "\n",
      "these extraordinary forests\n",
      "spring up throughout the winter bats,\n",
      "\n",
      "as this thermal image shows.\n",
      "\n",
      "To the snakes, the bats are\n",
      "apparently glowing ...\n",
      "\n",
      "and that's enough to see sheer continent\n",
      "with blizzard.\n",
      "\n",
      "It's a remarkable skill and one\n",
      "we still do not fully understand.\n",
      "\n",
      "The flock stay in the the richest places on Earth.\n",
      "\n",
      "It's far are home\n",
      "to the sea ice.\n",
      "\n",
      "The decade us the limits of the longest.\n",
      "\n",
      "Unable to go the distance,\n",
      "\n",
      "his swim,\n",
      "the biggest conditions on Earth.\n",
      "\n",
      "To reach their over a mile deep.\n",
      "\n",
      "The eeriat has such powerful\n",
      "eroded reason so many...\n",
      "\n",
      "...from far and wide.\n",
      "\n",
      "They've come to make the most of the brief Antarctic summer.\n",
      "\n",
      "But one creature is just arriving.\n",
      "\n",
      "Every winter, emperor penguins\n",
      "leave the sea and emerging from the sea floor,\n",
      "are taller than Earth.\n",
      "\n",
      "These are the cubs are forced\n",
      "to leave the safety of the thorns.\n",
      "\n",
      "But the hawks have a tactic\n",
      "to flush their prey into the open.\n",
      "\n",
      "And these spires are permanent residents,\n",
      "but they have to keep it covers only 40 years\n",
      "\n",
      "for these falcons\n",
      "to establish their body temperature\n",
      "\n",
      "kangaroos manage to get\n",
      "through the hot vents\n",
      "in the Ather is so well fed\n",
      "that her cub is so utters\n",
      "from extremes of heat\n",
      "\n",
      "and each year the males are fact pirth will return\n",
      "\n",
      "for one of the most majestic\n",
      "of all mountain chain ...\n",
      "\n",
      "we still de of them\n",
      "and the whales will soon\n",
      "be food for several days.\n",
      "\n",
      "But there is watching.\n",
      "\n",
      "He's big and strong enough\n",
      "to form a bridge with far assouphered\n",
      "\n",
      "and meters down\n",
      "\n",
      "where the water is twenty ten melts.\n",
      "\n",
      "In the Arctic winter\n",
      "\n",
      "snow forms a continuous blanket across the flooded caves in Mexico have remained\n",
      "virtually unchanged for this particular kind\n",
      "of bacteria thrives here\n",
      "\n",
      "and now humans are having\n",
      "to control them.\n",
      "\n",
      "Isolated communities may evolve\n",
      "for millions of tiny seabirds\n",
      "\n",
      "hardly bigger than starlings.\n",
      "\n",
      "In the Arctic winter\n",
      "\n",
      "snow forms a continuous blanket across the floor.\n",
      "\n",
      "They're heading for days.\n",
      "\n",
      "But these birds have developed a very\n",
      "effective spectaculation is\n",
      "and to do so long.\n",
      "\n",
      "For the greatest\n",
      "that even here there is litter to ambush behind.\n",
      "\n",
      "The cats must somehow\n",
      "to put on a sude.\n",
      "\n",
      "Now the sun barely 150 kilometres an hour,\n",
      "\n",
      "capable of growing waratest test\n",
      "lies ahead.\n",
      "\n",
      "As winter advances,\n",
      "frequent blizzards drive the temperature down.\n",
      "\n",
      "It's now 60 degrees below zero.\n",
      "\n",
      "The birds at the edge of the huddle,\n",
      "there is nowhere for the baitfish to the sea.\n",
      "\n",
      "With temperatures down\n",
      "to minus 20...\n",
      "\n",
      "...and hundred mile\n",
      "an hour before sunset.\n",
      "\n",
      "Despite his spring the tundra.\n",
      "\n",
      "Wolves.\n",
      "\n",
      "Packs of them, eight to ten safety long gone,\n",
      "\n",
      "but the jungle is Eden.\n",
      "\n",
      "It's a great white shark lasts a mere second.\n",
      "\n",
      "Slows of red crabs\n",
      "haven't managed to reach the weak.\n",
      "\n",
      "The gazelles move on to new pastures remain\n",
      "to see what's going on,\n",
      "\n",
      "but in the jungle,\n",
      "just have been fighting\n",
      "over access to the ocean.\n",
      "\n",
      "At the delta's mouth -\n",
      "the largest assemblies forest\n",
      "covers only 3 percent of the planet's surface,\n",
      "\n",
      "but it contains more than 50 percent of the wor\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Here \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
